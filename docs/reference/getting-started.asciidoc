[[getting-started]]
= 入门

[partintro]
--

Elasticsearch is a highly scalable open-source full-text search and analytics engine. It allows you to store, search, and analyze big volumes of data quickly and in near real time. It is generally used as the underlying engine/technology that powers applications that have complex search features and requirements.

Here are a few sample use-cases that Elasticsearch could be used for:

* You run an online web store where you allow your customers to search for products that you sell. In this case, you can use Elasticsearch to store your entire product catalog and inventory and provide search and autocomplete suggestions for them.
* You want to collect log or transaction data and you want to analyze and mine this data to look for trends, statistics, summarizations, or anomalies. In this case, you can use Logstash (part of the Elasticsearch/Logstash/Kibana stack) to collect, aggregate, and parse your data, and then have Logstash feed this data into Elasticsearch. Once the data is in Elasticsearch, you can run searches and aggregations to mine any information that is of interest to you.
* You run a price alerting platform which allows price-savvy customers to specify a rule like "I am interested in buying a specific electronic gadget and I want to be notified if the price of gadget falls below $X from any vendor within the next month". In this case you can scrape vendor prices, push them into Elasticsearch and use its reverse-search (Percolator) capability to match price movements against customer queries and eventually push the alerts out to the customer once matches are found.
* You have analytics/business-intelligence needs and want to quickly investigate, analyze, visualize, and ask ad-hoc questions on a lot of data (think millions or billions of records). In this case, you can use Elasticsearch to store your data and then use Kibana (part of the Elasticsearch/Logstash/Kibana stack) to build custom dashboards that can visualize aspects of your data that are important to you. Additionally, you can use the Elasticsearch aggregations functionality to perform complex business intelligence queries against your data.

For the rest of this tutorial, you will be guided through the process of getting Elasticsearch up and running, taking a peek inside it, and performing basic operations like indexing, searching, and modifying your data. At the end of this tutorial, you should have a good idea of what Elasticsearch is, how it works, and hopefully be inspired to see how you can use it to either build sophisticated search applications or to mine intelligence from your data.
--

[[getting-started-concepts]]
== Basic Concepts

There are a few concepts that are core to Elasticsearch. Understanding these concepts from the outset will tremendously help ease the learning process.

[float]
=== Near Realtime (NRT)

Elasticsearch is a near-realtime search platform. What this means is there is a slight latency (normally one second) from the time you index a document until the time it becomes searchable.

[float]
=== Cluster

A cluster is a collection of one or more nodes (servers) that together holds your entire data and provides federated indexing and search capabilities across all nodes. A cluster is identified by a unique name which by default is "elasticsearch". This name is important because a node can only be part of a cluster if the node is set up to join the cluster by its name.

Make sure that you don't reuse the same cluster names in different
environments, otherwise you might end up with nodes joining the wrong cluster.
For instance you could use `logging-dev`, `logging-stage`, and `logging-prod`
for the development, staging, and production clusters.

Note that it is valid and perfectly fine to have a cluster with only a single node in it. Furthermore, you may also have multiple independent clusters each with its own unique cluster name.

[float]
=== Node

A node is a single server that is part of your cluster, stores your data, and participates in the cluster's indexing and search
capabilities. Just like a cluster, a node is identified by a name which by default is a random Universally Unique IDentifier (UUID) that is assigned to the node at startup. You can define any node name you want if you do not want the default.  This name is important for administration purposes where you want to identify which servers in your network correspond to which nodes in your Elasticsearch cluster.

A node can be configured to join a specific cluster by the cluster name. By default, each node is set up to join a cluster named `elasticsearch` which means that if you start up a number of nodes on your network and--assuming they can discover each other--they will all automatically form and join a single cluster named `elasticsearch`.

In a single cluster, you can have as many nodes as you want. Furthermore, if there are no other Elasticsearch nodes currently running on your network, starting a single node will by default form a new single-node cluster named `elasticsearch`.

[float]
=== Index

An index is a collection of documents that have somewhat similar characteristics. For example, you can have an index for customer data, another index for a product catalog, and yet another index for order data. An index is identified by a name (that must be all lowercase) and this name is used to refer to the index when performing indexing, search, update, and delete operations against the documents in it.

In a single cluster, you can define as many indexes as you want.

[float]
=== Type

deprecated[6.0.0,See <<removal-of-types>>]

A type used to be a logical category/partition of your index to allow you to store different types of documents in the same index, e.g. one type for users, another type for blog posts.  It is no longer possible to create multiple types in an index, and the whole concept of types will be removed in a later version.  See <<removal-of-types>> for more.

[float]
=== Document

A document is a basic unit of information that can be indexed. For example, you can have a document for a single customer, another document for a single product, and yet another for a single order. This document is expressed in http://json.org/[JSON] (JavaScript Object Notation) which is a ubiquitous internet data interchange format. Within an index, you can store as many documents as you want.

[[getting-started-shards-and-replicas]]
[float]
=== Shards & Replicas

An index can potentially store a large amount of data that can exceed the hardware limits of a single node. For example, a single index of a billion documents taking up 1TB of disk space may not fit on the disk of a single node or may be too slow to serve search requests from a single node alone.

To solve this problem, Elasticsearch provides the ability to subdivide your index into multiple pieces called shards. When you create an index, you can simply define the number of shards that you want. Each shard is in itself a fully-functional and independent "index" that can be hosted on any node in the cluster.

Sharding is important for two primary reasons:

* It allows you to horizontally split/scale your content volume
* It allows you to distribute and parallelize operations across shards (potentially on multiple nodes) thus increasing performance/throughput


The mechanics of how a shard is distributed and also how its documents are aggregated back into search requests are completely managed by Elasticsearch and is transparent to you as the user.

In a network/cloud environment where failures can be expected anytime, it is very useful and highly recommended to have a failover mechanism in case a shard/node somehow goes offline or disappears for whatever reason. To this end, Elasticsearch allows you to make one or more copies of your index's shards into what are called replica shards, or replicas for short.

Replication is important for two primary reasons:

* It provides high availability in case a shard/node fails. For this reason, it is important to note that a replica shard is never allocated on the same node as the original/primary shard that it was copied from.
* It allows you to scale out your search volume/throughput since searches can be executed on all replicas in parallel.


To summarize, each index can be split into multiple shards. An index can also be replicated zero (meaning no replicas) or more times. Once replicated, each index will have primary shards (the original shards that were replicated from) and replica shards (the copies of the primary shards).

The number of shards and replicas can be defined per index at the time the index is created. After the index is created, you may also change the number of replicas dynamically anytime. You can change the number of shards for an existing index using the {ref}/indices-shrink-index.html[`_shrink`] and {ref}/indices-split-index.html[`_split`] APIs, however this is not a trivial task and pre-planning for the correct number of shards is the optimal approach.

By default, each index in Elasticsearch is allocated one primary shard and one replica which means that if you have at least two nodes in your cluster, your index will have one primary shard and another replica shard (one complete replica) for a total of two shards per index.

NOTE: Each Elasticsearch shard is a Lucene index.  There is a maximum number of documents you can have in a single Lucene index.  As of https://issues.apache.org/jira/browse/LUCENE-5843[`LUCENE-5843`], the limit is `2,147,483,519` (= Integer.MAX_VALUE - 128) documents.
You can monitor shard sizes using the {ref}/cat-shards.html[`_cat/shards`] API.

With that out of the way, let's get started with the fun part...

[[getting-started-install]]
== Installation

[TIP]
==============
You can skip having to install Elasticsearch by using our
https://www.elastic.co/cloud/elasticsearch-service[hosted Elasticsearch Service]
on Elastic Cloud. The Elasticsearch Service is available on both AWS and GCP.
https://www.elastic.co/cloud/elasticsearch-service/signup[Try out the
Elasticsearch Service for free].
==============

Elasticsearch requires at least Java 8. Specifically as of this writing, it is recommended that you use the Oracle JDK version {jdk}. Java installation varies from platform to platform so we won't go into those details here. Oracle's recommended installation documentation can be found on http://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html[Oracle's website]. Suffice to say, before you install Elasticsearch, please check your Java version first by running (and then install/upgrade accordingly if needed):

[source,sh]
--------------------------------------------------
java -version
echo $JAVA_HOME
--------------------------------------------------

Once we have Java set up, we can then download and run Elasticsearch. The binaries are available from http://www.elastic.co/downloads[`www.elastic.co/downloads`] along with all the releases that have been made in the past. For each release, you have a choice among a `zip` or `tar` archive, a `DEB` or `RPM` package, or a Windows `MSI` installation package.

[float]
=== Installation example with tar

For simplicity, let's use the {ref}/zip-targz.html[tar] file.

Let's download the Elasticsearch {version} tar as follows:

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-{version}.tar.gz
--------------------------------------------------
// NOTCONSOLE

Then extract it as follows:

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
tar -xvf elasticsearch-{version}.tar.gz
--------------------------------------------------

It will then create a bunch of files and folders in your current directory. We then go into the bin directory as follows:

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
cd elasticsearch-{version}/bin
--------------------------------------------------

And now we are ready to start our node and single cluster:

[source,sh]
--------------------------------------------------
./elasticsearch
--------------------------------------------------

[float]
=== Installation example with MSI Windows Installer

For Windows users, we recommend using the {ref}/windows.html[MSI Installer package]. The package contains a graphical user interface (GUI) that guides you through the installation process.

First, download the Elasticsearch {version} MSI from
https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-{version}.msi.

Then double-click the downloaded file to launch the GUI. Within the first screen, select the deployment directories:

[[getting-started-msi-installer-locations]]
image::images/msi_installer/msi_installer_locations.png[]

Then select whether to install as a service or start Elasticsearch manually as needed.
To align with the tar example, choose not to install as a service:

[[getting-started-msi-installer-service]]
image::images/msi_installer/msi_installer_no_service.png[]

For configuration, simply leave the default values:

[[getting-started-msi-installer-configuration]]
image::images/msi_installer/msi_installer_configuration.png[]

Again, to align with the tar example, uncheck all plugins to not install any plugins:

[[getting-started-msi-installer-plugins]]
image::images/msi_installer/msi_installer_plugins.png[]

After clicking the install button, Elasticsearch will be installed:

[[getting-started-msi-installer-success]]
image::images/msi_installer/msi_installer_success.png[]

By default, Elasticsearch will be installed at `%PROGRAMFILES%\Elastic\Elasticsearch`. Navigate here and go into the bin directory as follows:

**with Command Prompt:**

[source,sh]
--------------------------------------------------
cd %PROGRAMFILES%\Elastic\Elasticsearch\bin
--------------------------------------------------

**with PowerShell:**

[source,powershell]
--------------------------------------------------
cd $env:PROGRAMFILES\Elastic\Elasticsearch\bin
--------------------------------------------------

And now we are ready to start our node and single cluster:

[source,sh]
--------------------------------------------------
.\elasticsearch.exe
--------------------------------------------------

[float]
[[successfully-running-node]]
=== Successfully running node

If everything goes well with installation, you should see a bunch of messages that look like below:

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
[2018-09-13T12:20:01,766][INFO ][o.e.e.NodeEnvironment    ] [localhost.localdomain] using [1] data paths, mounts [[/home (/dev/mapper/fedora-home)]], net usable_space [335.3gb], net total_space [410.3gb], types [ext4]
[2018-09-13T12:20:01,772][INFO ][o.e.e.NodeEnvironment    ] [localhost.localdomain] heap size [990.7mb], compressed ordinary object pointers [true]
[2018-09-13T12:20:01,774][INFO ][o.e.n.Node               ] [localhost.localdomain] node name [localhost.localdomain], node ID [B0aEHNagTiWx7SYj-l4NTw]
[2018-09-13T12:20:01,775][INFO ][o.e.n.Node               ] [localhost.localdomain] version[{version}], pid[13030], build[oss/zip/77fc20e/2018-09-13T15:37:57.478402Z], OS[Linux/4.16.11-100.fc26.x86_64/amd64], JVM["Oracle Corporation"/OpenJDK 64-Bit Server VM/10/10+46]
[2018-09-13T12:20:01,775][INFO ][o.e.n.Node               ] [localhost.localdomain] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/tmp/elasticsearch.LN1ctLCi, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Djava.locale.providers=COMPAT, -XX:UseAVX=2, -Dio.netty.allocator.type=unpooled, -Des.path.home=/home/manybubbles/Workspaces/Elastic/master/elasticsearch/qa/unconfigured-node-name/build/cluster/integTestCluster node0/elasticsearch-7.0.0-alpha1-SNAPSHOT, -Des.path.conf=/home/manybubbles/Workspaces/Elastic/master/elasticsearch/qa/unconfigured-node-name/build/cluster/integTestCluster node0/elasticsearch-7.0.0-alpha1-SNAPSHOT/config, -Des.distribution.flavor=oss, -Des.distribution.type=zip]
[2018-09-13T12:20:02,543][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [aggs-matrix-stats]
[2018-09-13T12:20:02,543][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [analysis-common]
[2018-09-13T12:20:02,543][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [ingest-common]
[2018-09-13T12:20:02,544][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [lang-expression]
[2018-09-13T12:20:02,544][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [lang-mustache]
[2018-09-13T12:20:02,544][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [lang-painless]
[2018-09-13T12:20:02,544][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [mapper-extras]
[2018-09-13T12:20:02,544][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [parent-join]
[2018-09-13T12:20:02,544][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [percolator]
[2018-09-13T12:20:02,544][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [rank-eval]
[2018-09-13T12:20:02,544][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [reindex]
[2018-09-13T12:20:02,545][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [repository-url]
[2018-09-13T12:20:02,545][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] loaded module [transport-netty4]
[2018-09-13T12:20:02,545][INFO ][o.e.p.PluginsService     ] [localhost.localdomain] no plugins loaded
[2018-09-13T12:20:04,657][INFO ][o.e.d.DiscoveryModule    ] [localhost.localdomain] using discovery type [zen]
[2018-09-13T12:20:05,006][INFO ][o.e.n.Node               ] [localhost.localdomain] initialized
[2018-09-13T12:20:05,007][INFO ][o.e.n.Node               ] [localhost.localdomain] starting ...
[2018-09-13T12:20:05,202][INFO ][o.e.t.TransportService   ] [localhost.localdomain] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
[2018-09-13T12:20:05,221][WARN ][o.e.b.BootstrapChecks    ] [localhost.localdomain] max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]
[2018-09-13T12:20:05,221][WARN ][o.e.b.BootstrapChecks    ] [localhost.localdomain] max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
[2018-09-13T12:20:08,355][INFO ][o.e.c.s.MasterService    ] [localhost.localdomain] elected-as-master ([0] nodes joined)[, ], reason: master node changed {previous [], current [{localhost.localdomain}{B0aEHNagTiWx7SYj-l4NTw}{hzsQz6CVQMCTpMCVLM4IHg}{127.0.0.1}{127.0.0.1:9300}{testattr=test}]}
[2018-09-13T12:20:08,360][INFO ][o.e.c.s.ClusterApplierService] [localhost.localdomain] master node changed {previous [], current [{localhost.localdomain}{B0aEHNagTiWx7SYj-l4NTw}{hzsQz6CVQMCTpMCVLM4IHg}{127.0.0.1}{127.0.0.1:9300}{testattr=test}]}, reason: apply cluster state (from master [master {localhost.localdomain}{B0aEHNagTiWx7SYj-l4NTw}{hzsQz6CVQMCTpMCVLM4IHg}{127.0.0.1}{127.0.0.1:9300}{testattr=test} committed version [1] source [elected-as-master ([0] nodes joined)[, ]]])
[2018-09-13T12:20:08,384][INFO ][o.e.h.n.Netty4HttpServerTransport] [localhost.localdomain] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
[2018-09-13T12:20:08,384][INFO ][o.e.n.Node               ] [localhost.localdomain] started

--------------------------------------------------

Without going too much into detail, we can see that our node named "6-bjhwl" (which will be a different set of characters in your case) has started and elected itself as a master in a single cluster. Don't worry yet at the moment what master means. The main thing that is important here is that we have started one node within one cluster.

As mentioned previously, we can override either the cluster or node name. This can be done from the command line when starting Elasticsearch as follows:

[source,sh]
--------------------------------------------------
./elasticsearch -Ecluster.name=my_cluster_name -Enode.name=my_node_name
--------------------------------------------------

Also note the line marked http with information about the HTTP address (`192.168.8.112`) and port (`9200`) that our node is reachable from. By default, Elasticsearch uses port `9200` to provide access to its REST API. This port is configurable if necessary.

[[getting-started-explore]]
== 探索您的群集

[float]
=== The REST API

现在我们已经启动并运行了节点（和集群），下一步是了解如何与之通信。幸运的是，Elasticsearch提供了一个非常全面和强大的REST API，您可以使用它与集群进行交互。使用API可以完成的一些事项如下：

* 检查群集，节点和索引运行状况，状态和统计信息
* 管理您的群集，节点和索引数据和元数据
* 对索引执行CRUD（创建，读取，更新和删除）和搜索操作
* 执行高级搜索操作，例如分页，排序，过滤，脚本编写，聚合等等

[[getting-started-cluster-health]]
=== 群集健康

让我们从基本运行状况检查开始，我们可以使用它来查看集群的运行情况。我们将使用curl来执行此操作，但您可以使用任何允许您进行HTTP/REST调用的工具。假设我们仍然在我们启动Elasticsearch的同一节点上打开另一个命令shell窗口。

You can
run the command below in {kibana-ref}/console-kibana.html[Kibana's Console]
by clicking "VIEW IN CONSOLE" or with `curl` by clicking the "COPY AS CURL"
link below and pasting it into a terminal.
要检查群集运行状况，我们将使用{ref}/cat.html [`_cat` API]。您可以通过单击“VIEW IN CONSOLE”在{kibana-ref}/console-kibana.html[Kibana的控制台]中查看或单击下面的“COPY AS CURL”链接并将其粘贴到终端中使用`curl`查看，。

[source,js]
--------------------------------------------------
GET /_cat/health?v
--------------------------------------------------
// CONSOLE

响应:

[source,txt]
--------------------------------------------------
epoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1475247709 17:01:49  elasticsearch green           1         1      0   0    0    0        0             0                  -                100.0%
--------------------------------------------------
// TESTRESPONSE[s/1475247709 17:01:49  elasticsearch/\\d+ \\d+:\\d+:\\d+ docs_integTestCluster/]
// TESTRESPONSE[s/0             0                  -/0             \\d+                  -/]
// TESTRESPONSE[_cat]

我们可以看到名为“elasticsearch”的集群处于绿色状态。

每当我们要查看集群健康时，我们得到green（正常）, yellow（警告）, red（严重）三种状态。

    * Green - 一切都很好（集群功能齐全）
    * Yellow - 所有数据都可用，但尚未分配一些副本（群集功能齐全）
    * Red - 某些数据由于某种原因不可用（群集部分功能）

**注意:** 当群集为红色时，它将继续提供来自可用分片的搜索请求，但您可能需要尽快修复它，因为存在未分配的分片。

同样从上面的响应中，我们可以看到总共1个节点，并且我们有0个分片，因为集群中还没有数据。请注意，由于我们使用默认群集名称（elasticsearch），并且由于Elasticsearch默认使用单播网络发现（unicast network discovery）来查找同一台计算机上的其他节点，因此您可能会意外启动计算机上的多个节点并让它们都加入一个单一集群。在这种情况下，您可能会在上面的响应中看到多个节点。

我们还可以获得群集中的节点列表，如下所示:

[source,js]
--------------------------------------------------
GET /_cat/nodes?v
--------------------------------------------------
// CONSOLE

响应:

[source,txt]
--------------------------------------------------
ip        heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
127.0.0.1           10           5   5    4.46                        mdi      *      PB2SGZY
--------------------------------------------------
// TESTRESPONSE[s/10           5   5    4.46/\\d+ \\d+ \\d+ (\\d+\\.\\d+)? (\\d+\\.\\d+)? (\\d+\.\\d+)?/]
// TESTRESPONSE[s/[*]/[*]/ s/PB2SGZY/.+/ _cat]

在这里，我们可以看到一个名为“PB2SGZY”的节点，它是当前在我们集群中的单个节点。

[[getting-started-list-indices]]
=== 列出所有索引（Indices）

现在让我们来看看我们的索引：

[source,js]
--------------------------------------------------
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE

响应:

[source,txt]
--------------------------------------------------
health status index uuid pri rep docs.count docs.deleted store.size pri.store.size
--------------------------------------------------
// TESTRESPONSE[_cat]

这仅仅意味着我们在集群中还没有索引。

[[getting-started-create-index]]
=== 创建索引（Index）

现在让我们创建一个名为“customer”的索引，然后再次列出所有索引：

[source,js]
--------------------------------------------------
PUT /customer?pretty
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE

第一个命令使用PUT动词创建名为“customer”的索引。我们只是追加 `pretty` 到调用的末尾，告诉它打印美化格式的JSON响应（如果有的话）。

响应:

[source,txt]
--------------------------------------------------
health status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   customer 95SQ4TSUT7mWBT7VNHH67A   1   1          0            0       260b           260b
--------------------------------------------------
// TESTRESPONSE[s/95SQ4TSUT7mWBT7VNHH67A/.+/ s/260b/\\d+\\.?\\d?k?b/ _cat]

第二个命令的结果告诉我们，我们现在有一个名为customer的索引，它有一个主分片（primary shard）和一个副本（replica，默认值），里面没有任何文档。

您可能还注意到客户索引标记了黄色运行状况。回想一下我们之前的讨论，黄色表示某些副本尚未分配。此索引发生这种情况的原因是因为默认情况下Elasticsearch为此索引创建了一个副本。由于我们目前只有一个节点在运行，因此在另一个节点加入集群之前，尚无法分配一个副本（用于高可用性）。一旦该副本分配到第二个节点，此索引的运行状况将变为绿色。

[[getting-started-query-document]]
=== 索引和查询文档

现在让我们在客户索引（customer）中加入一些内容。我们将一个简单的客户文档索引到客户索引中，ID为1，如下所示：

[source,js]
--------------------------------------------------
PUT /customer/_doc/1?pretty
{
  "name": "John Doe"
}
--------------------------------------------------
// CONSOLE

响应:

[source,js]
--------------------------------------------------
{
  "_index" : "customer",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}
--------------------------------------------------
// TESTRESPONSE[s/"_seq_no" : \d+/"_seq_no" : $body._seq_no/ s/"_primary_term" : 1/"_primary_term" : $body._primary_term/]

从上面可以看出，在客户索引(customer)中成功创建了一个新的客户文档。该文档还具有我们在索引时指定的内部标识1。

值得注意的是，Elasticsearch不需要在将文档编入索引之前先显式创建索引。在前面的示例中，如果客户索引事先尚未存在，则Elasticsearch将自动创建客户索引。

我们现在检索刚刚编入索引的文档：

[source,js]
--------------------------------------------------
GET /customer/_doc/1?pretty
--------------------------------------------------
// CONSOLE
// TEST[continued]

响应:

[source,js]
--------------------------------------------------
{
  "_index" : "customer",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "_seq_no" : 25,
  "_primary_term" : 1,
  "found" : true,
  "_source" : { "name": "John Doe" }
}
--------------------------------------------------
// TESTRESPONSE[s/"_seq_no" : \d+/"_seq_no" : $body._seq_no/ s/"_primary_term" : 1/"_primary_term" : $body._primary_term/]

除了字段 `found` 之外，这里没有任何不同，说明我们找到了一个具有请求的ID 1和另一个字段 `_source` 的文档，它返回我们从上一步索引的完整JSON文档。

[[getting-started-delete-index]]
=== 删除索引

现在让我们删除刚刚创建的索引，然后再次列出所有索引：

[source,js]
--------------------------------------------------
DELETE /customer?pretty
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE
// TEST[continued]

And the response:

[source,txt]
--------------------------------------------------
health status index uuid pri rep docs.count docs.deleted store.size pri.store.size
--------------------------------------------------
// TESTRESPONSE[_cat]

这意味着索引已成功删除，现在我们回到了我们启动集群时没有任何内容的时候。

在我们继续往下看之前，让我们再仔细看看到目前为止我们学到的一些API命令：

[source,js]
--------------------------------------------------
PUT /customer
PUT /customer/_doc/1
{
  "name": "John Doe"
}
GET /customer/_doc/1
DELETE /customer
--------------------------------------------------
// CONSOLE

如果我们仔细研究上述命令，我们实际上可以看到如何在Elasticsearch中访问数据的模式。该模式可归纳如下：

[source,js]
--------------------------------------------------
<HTTP Verb> /<Index>/<Endpoint>/<ID>
--------------------------------------------------
// NOTCONSOLE

这种REST访问模式在所有API命令中都非常普遍，如果您能够简单地记住它，您将在掌握Elasticsearch方面有一个良好的开端。

[[getting-started-modify-data]]
== 修改您的数据

Elasticsearch几乎实时提供数据操作和搜索功能。默认情况下，从开始索引/更新/删除数据到在搜索结果中显示，您可能会有一秒钟的延迟（刷新间隔）。这是与SQL（数据在事务完成后立即可用）等其他平台的重要区别。

[float]
=== 索引/替换 文档

我们之前已经看到了如何索引单个文档。让我们再次回忆一下这个命令：

[source,js]
--------------------------------------------------
PUT /customer/_doc/1?pretty
{
  "name": "John Doe"
}
--------------------------------------------------
// CONSOLE

同样，上面将指定的文档索引到customer索引中，ID为1.如果我们再使用不同（或相同）的文档执行上述命令，Elasticsearch将在ID为1的现有文档之上替换（即重新索引）成新文档：

[source,js]
--------------------------------------------------
PUT /customer/_doc/1?pretty
{
  "name": "Jane Doe"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

以上内容将ID为1的文档名称从“John Doe”更改为“Jane Doe”。另一方面，如果我们使用不同的ID，则将索引新文档，并且索引中已有的现有文档保持不变。

[source,js]
--------------------------------------------------
PUT /customer/_doc/2?pretty
{
  "name": "Jane Doe"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

以上将索引一个ID为2的新文档。

索引时，ID部分是可选的。如果未指定，Elasticsearch将生成随机ID，然后使用它来索引文档。Elasticsearch生成的实际ID（或前面示例中显式指定的内容）将作为索引API调用的一部分返回。

此示例显示如何在没有显式ID的情况下索引文档：

[source,js]
--------------------------------------------------
POST /customer/_doc?pretty
{
  "name": "Jane Doe"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

请注意，在上面的情况中，我们使用`POST` 动词而不是PUT，因为我们没有指定ID。

[[getting-started-update-documents]]
=== 更新文档

除了能够索引和替换文档，我们还可以更新文档。请注意，Elasticsearch实际上并没有进行就地更新。每当我们进行更新时，Elasticsearch都会删除旧文档，然后一次性应用更新索引新文档。

此示例演示如何通过将名称字段更改为“Jane Doe”来更新以前的文档（ID为1）：

[source,js]
--------------------------------------------------
POST /customer/_update/1?pretty
{
  "doc": { "name": "Jane Doe" }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

此示例演示如何通过将名称字段更改为“Jane Doe”来更新我们以前的文档（ID为1），同时为其添加年龄字段：

[source,js]
--------------------------------------------------
POST /customer/_update/1?pretty
{
  "doc": { "name": "Jane Doe", "age": 20 }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

也可以使用简单脚本执行更新。此示例使用脚本将年龄增加5：

[source,js]
--------------------------------------------------
POST /customer/_update/1?pretty
{
  "script" : "ctx._source.age += 5"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

在上面的示例中，`ctx._source` 指的是当前即将更新的源文档。

Elasticsearch provides the ability to update multiple documents given a query condition (like an `SQL UPDATE-WHERE` statement). See {ref}/docs-update-by-query.html[`docs-update-by-query` API]
Elasticsearch提供了在给定查询条件（就像SQL UPDATE-WHERE语句）的情况下更新多个文档的功能。参见{ref}/docs-update-by-query.html[`docs-update-by-query` API]

[[getting-started-delete-documents]]
=== 删除文档

删除文档非常简单。此示例显示如何删除ID为2的以前的customer索引：

[source,js]
--------------------------------------------------
DELETE /customer/_doc/2?pretty
--------------------------------------------------
// CONSOLE
// TEST[continued]

请参阅{ref}/docs-delete-by-query.html[`_delete_by_query` API]来删除与特定查询匹配的所有文档。值得注意的是，删除整个索引而不是使用Delete By Query API删除所有文档会更有效。

[[getting-started-batch-processing]]
=== 批量处理

除了能够索引，更新和删除单个文档之外，Elasticsearch还提供了使用{ref}/docs-bulk.html[`_bulk` API] 批量执行上述任何操作的功能。此功能非常重要，因为它提供了一种非常有效的机制，可以尽可能快地进行多个操作，并尽可能少地进行网络往返。

作为一个简单的示例，以下调用在一个批量操作中索引两个文档（ID 1 - John Doe和ID 2 - Jane Doe）：

[source,js]
--------------------------------------------------
POST /customer/_bulk?pretty
{"index":{"_id":"1"}}
{"name": "John Doe" }
{"index":{"_id":"2"}}
{"name": "Jane Doe" }
--------------------------------------------------
// CONSOLE

此示例更新第一个文档（ID为1），然后在一个批量操作中删除第二个文档（ID为2）：

[source,sh]
--------------------------------------------------
POST /customer/_bulk?pretty
{"update":{"_id":"1"}}
{"doc": { "name": "John Doe becomes Jane Doe" } }
{"delete":{"_id":"2"}}
--------------------------------------------------
// CONSOLE
// TEST[continued]

请注意，对于删除操作，之后没有相应的源文档，因为删除只需要删除文档的ID。

Bulk API不会因其中一个操作失败而失败。如果单个操作因任何原因失败，它将继续处理其后的其余操作。批量API返回时，它将为每个操作提供一个状态（按照发送的顺序），以便您可以检查特定操作是否失败。

[[getting-started-explore-data]]
== 探索您的数据

[float]
=== 样本数据集

现在我们已经了解了基础知识，让我们尝试更真实的数据集。我准备了一份关于客户银行账户信息的虚构JSON文档样本。每个文档都有以下架构：

[source,js]
--------------------------------------------------
{
    "account_number": 0,
    "balance": 16623,
    "firstname": "Bradshaw",
    "lastname": "Mckenzie",
    "age": 29,
    "gender": "F",
    "address": "244 Columbus Place",
    "employer": "Euron",
    "email": "bradshawmckenzie@euron.com",
    "city": "Hobucken",
    "state": "CO"
}
--------------------------------------------------
// NOTCONSOLE

好奇的是，这些数据是使用http://www.json-generator.com/[`www.json-generator.com/`]生成的，因此请忽略数据的实际值和语义，因为这些都是随机生成的。

[float]
=== 加载示例数据集

您可以从https://github.com/elastic/elasticsearch/blob/master/docs/src/test/resources/accounts.json?raw=true[此处]下载样本数据集（accounts.json）。将它解压缩到我们当前的目录，然后将它加载到我们的集群中，如下所示：

[source,sh]
--------------------------------------------------
curl -H "Content-Type: application/json" -XPOST "localhost:9200/bank/_bulk?pretty&refresh" --data-binary "@accounts.json"
curl "localhost:9200/_cat/indices?v"
--------------------------------------------------
// NOTCONSOLE

////
This replicates the above in a document-testing friendly way but isn't visible
in the docs:

[source,js]
--------------------------------------------------
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE
// TEST[setup:bank]
////

响应:

[source,txt]
--------------------------------------------------
health status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   bank  l7sSYV2cQXmu6_4rJWVIww   5   1       1000            0    128.6kb        128.6kb
--------------------------------------------------
// TESTRESPONSE[s/128.6kb/\\d+(\\.\\d+)?[mk]?b/]
// TESTRESPONSE[s/l7sSYV2cQXmu6_4rJWVIww/.+/ _cat]

这意味着我们只是成功地将1000个文档批量索引到bank索引中。

[[getting-started-search-API]]
=== 搜索API

现在让我们从一些简单的搜索开始吧。运行搜索有两种基本方法：一种是通过{ref}/search-uri-request.html[REST request URI]发送搜索参数，另一种是通过{ref}/search-request-body.html[REST request body]发送。请求体方法允许您更具表现力，并以更可读的JSON格式定义搜索。我们将尝试一个请求URI方法的示例，但是对于本教程的其余部分，我们将专门使用请求体方法。

可以从_search端点访问用于搜索的REST API 。此示例返回bank索引中的所有文档：

[source,js]
--------------------------------------------------
GET /bank/_search?q=*&sort=account_number:asc&pretty
--------------------------------------------------
// CONSOLE
// TEST[continued]

让我们首先剖析搜索调用。我们正在index索引中搜索（`_search` 端点），`q=*` 参数指示Elasticsearch匹配索引中的所有文档。 `sort=account_number:asc` 参数指示每个文档使用 `account_number` 字段以升序对结果进行排序。 `pretty` 参数再次告诉Elasticsearch返回漂亮的JSON结果。

响应（部分显示）：

[source,js]
--------------------------------------------------
{
  "took" : 63,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
        "value": 1000,
        "relation": "eq"
    },
    "max_score" : null,
    "hits" : [ {
      "_index" : "bank",
      "_type" : "_doc",
      "_id" : "0",
      "sort": [0],
      "_score" : null,
      "_source" : {"account_number":0,"balance":16623,"firstname":"Bradshaw","lastname":"Mckenzie","age":29,"gender":"F","address":"244 Columbus Place","employer":"Euron","email":"bradshawmckenzie@euron.com","city":"Hobucken","state":"CO"}
    }, {
      "_index" : "bank",
      "_type" : "_doc",
      "_id" : "1",
      "sort": [1],
      "_score" : null,
      "_source" : {"account_number":1,"balance":39225,"firstname":"Amber","lastname":"Duke","age":32,"gender":"M","address":"880 Holmes Lane","employer":"Pyrami","email":"amberduke@pyrami.com","city":"Brogan","state":"IL"}
    }, ...
    ]
  }
}
--------------------------------------------------
// TESTRESPONSE[s/"took" : 63/"took" : $body.took/]
// TESTRESPONSE[s/\.\.\./$body.hits.hits.2, $body.hits.hits.3, $body.hits.hits.4, $body.hits.hits.5, $body.hits.hits.6, $body.hits.hits.7, $body.hits.hits.8, $body.hits.hits.9/]

对于响应，我们看到以下部分：

* `took` – Elasticsearch执行搜索的时间（以毫秒为单位）
* `timed_out` – 告诉我们搜索是否超时
* `_shards` – 告诉我们搜索了多少个分片，以及搜索成功/失败分片的计数
* `hits` – 搜索结果
* `hits.total` – 包含与我们的搜索条件匹配的文档总数的信息对象
** `hits.total.value` - 总命中数的值（必须在上下文hits.total.relation中解释执行）。
** `hits.total.relation` - `hits.total.value` 是否是确切的命中计数，在这种情况下它等于"eq"或总命中数的下限（大于或等于），在这种情况下它等于gte。
* `hits.hits` – 实际的搜索结果数组（默认最前面10个文档）
* `hits.sort` - 对结果进行排序键（如果按score排序则此字段不存在）sort key for results (missing if sorting by score)
* `hits._score` and `max_score` - ignore these fields for now

The accuracy of `hits.total` is controlled by the request parameter `track_total_hits`, when set to true
the request will track the total hits accurately (`"relation": "eq"`). It defaults to `10,000`
which means that the total hit count is accurately tracked up to `10,000` documents.
You can force an accurate count by setting `track_total_hits` to true explicitly.
See the <<search-request-track-total-hits, request body>> documentation
for more details.

Here is the same exact search above using the alternative request body method:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "sort": [
    { "account_number": "asc" }
  ]
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

The difference here is that instead of passing `q=*` in the URI, we provide a JSON-style query request body to the `_search` API. We'll discuss this JSON query in the next section.

////
Hidden response just so we can assert that it is indeed the same but don't have
to clutter the docs with it:

[source,js]
--------------------------------------------------
{
  "took" : 63,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
       "value": 1000,
       "relation": "eq"
    },
    "max_score": null,
    "hits" : [ {
      "_index" : "bank",
      "_type" : "_doc",
      "_id" : "0",
      "sort": [0],
      "_score": null,
      "_source" : {"account_number":0,"balance":16623,"firstname":"Bradshaw","lastname":"Mckenzie","age":29,"gender":"F","address":"244 Columbus Place","employer":"Euron","email":"bradshawmckenzie@euron.com","city":"Hobucken","state":"CO"}
    }, {
      "_index" : "bank",
      "_type" : "_doc",
      "_id" : "1",
      "sort": [1],
      "_score": null,
      "_source" : {"account_number":1,"balance":39225,"firstname":"Amber","lastname":"Duke","age":32,"gender":"M","address":"880 Holmes Lane","employer":"Pyrami","email":"amberduke@pyrami.com","city":"Brogan","state":"IL"}
    }, ...
    ]
  }
}
--------------------------------------------------
// TESTRESPONSE[s/"took" : 63/"took" : $body.took/]
// TESTRESPONSE[s/\.\.\./$body.hits.hits.2, $body.hits.hits.3, $body.hits.hits.4, $body.hits.hits.5, $body.hits.hits.6, $body.hits.hits.7, $body.hits.hits.8, $body.hits.hits.9/]

////

It is important to understand that once you get your search results back, Elasticsearch is completely done with the request and does not maintain any kind of server-side resources or open cursors into your results. This is in stark contrast to many other platforms such as SQL wherein you may initially get a partial subset of your query results up-front and then you have to continuously go back to the server if you want to fetch (or page through) the rest of the results using some kind of stateful server-side cursor.

[[getting-started-query-lang]]
=== Introducing the Query Language

Elasticsearch provides a JSON-style domain-specific language that you can use to execute queries. This is referred to as the {ref}/query-dsl.html[Query DSL]. The query language is quite comprehensive and can be intimidating at first glance but the best way to actually learn it is to start with a few basic examples.

Going back to our last example, we executed this query:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Dissecting the above, the `query` part tells us what our query definition is and the `match_all` part is simply the type of query that we want to run. The `match_all` query is simply a search for all documents in the specified index.

In addition to the `query` parameter, we also can pass other parameters to
influence the search results. In the example in the section above we passed in
`sort`, here we pass in `size`:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "size": 1
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Note that if `size` is not specified, it defaults to 10.

This example does a `match_all` and returns documents 10 through 19:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "from": 10,
  "size": 10
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

The `from` parameter (0-based) specifies which document index to start from and the `size` parameter specifies how many documents to return starting at the from parameter. This feature is useful when implementing paging of search results. Note that if `from` is not specified, it defaults to 0.

This example does a `match_all` and sorts the results by account balance in descending order and returns the top 10 (default size) documents.

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "sort": { "balance": { "order": "desc" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

[[getting-started-search]]
=== Executing Searches

Now that we have seen a few of the basic search parameters, let's dig in some more into the Query DSL. Let's first take a look at the returned document fields. By default, the full JSON document is returned as part of all searches. This is referred to as the source (`_source` field in the search hits). If we don't want the entire source document returned, we have the ability to request only a few fields from within source to be returned.

This example shows how to return two fields, `account_number` and `balance` (inside of `_source`), from the search:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "_source": ["account_number", "balance"]
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Note that the above example simply reduces the `_source` field. It will still only return one field named `_source` but within it, only the fields `account_number` and `balance` are included.

If you come from a SQL background, the above is somewhat similar in concept to the `SQL SELECT FROM` field list.

Now let's move on to the query part. Previously, we've seen how the `match_all` query is used to match all documents. Let's now introduce a new query called the {ref}/query-dsl-match-query.html[`match` query], which can be thought of as a basic fielded search query (i.e. a search done against a specific field or set of fields).

This example returns the account numbered 20:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "account_number": 20 } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example returns all accounts containing the term "mill" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "address": "mill" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example returns all accounts containing the term "mill" or "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "address": "mill lane" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example is a variant of `match` (`match_phrase`) that returns all accounts containing the phrase "mill lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_phrase": { "address": "mill lane" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Let's now introduce the {ref}/query-dsl-bool-query.html[`bool` query]. The `bool` query allows us to compose smaller queries into bigger queries using boolean logic.

This example composes two `match` queries and returns all accounts containing "mill" and "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In the above example, the `bool must` clause specifies all the queries that must be true for a document to be considered a match.

In contrast, this example composes two `match` queries and returns all accounts containing "mill" or "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In the above example, the `bool should` clause specifies a list of queries either of which must be true for a document to be considered a match.

This example composes two `match` queries and returns all accounts that contain neither "mill" nor "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must_not": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In the above example, the `bool must_not` clause specifies a list of queries none of which must be true for a document to be considered a match.

We can combine `must`, `should`, and `must_not` clauses simultaneously inside a `bool` query. Furthermore, we can compose `bool` queries inside any of these `bool` clauses to mimic any complex multi-level boolean logic.

This example returns all accounts of anybody who is 40 years old but doesn't live in ID(aho):

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "age": "40" } }
      ],
      "must_not": [
        { "match": { "state": "ID" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

[[getting-started-filters]]
=== Executing Filters

In the previous section, we skipped over a little detail called the document score (`_score` field in the search results). The score is a numeric value that is a relative measure of how well the document matches the search query that we specified. The higher the score, the more relevant the document is, the lower the score, the less relevant the document is.

But queries do not always need to produce scores, in particular when they are only used for "filtering" the document set. Elasticsearch detects these situations and automatically optimizes query execution in order not to compute useless scores.

The {ref}/query-dsl-bool-query.html[`bool` query] that we introduced in the previous section also supports `filter` clauses which allow us to use a query to restrict the documents that will be matched by other clauses, without changing how scores are computed. As an example, let's introduce the {ref}/query-dsl-range-query.html[`range` query], which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.

This example uses a bool query to return all accounts with balances between 20000 and 30000, inclusive. In other words, we want to find accounts with a balance that is greater than or equal to 20000 and less than or equal to 30000.

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": { "match_all": {} },
      "filter": {
        "range": {
          "balance": {
            "gte": 20000,
            "lte": 30000
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Dissecting the above, the bool query contains a `match_all` query (the query part) and a `range` query (the filter part). We can substitute any other queries into the query and the filter parts. In the above case, the range query makes perfect sense since documents falling into the range all match "equally", i.e., no document is more relevant than another.

In addition to the `match_all`, `match`, `bool`, and `range` queries, there are a lot of other query types that are available and we won't go into them here. Since we already have a basic understanding of how they work, it shouldn't be too difficult to apply this knowledge in learning and experimenting with the other query types.

[[getting-started-aggregations]]
=== Executing Aggregations

Aggregations provide the ability to group and extract statistics from your data. The easiest way to think about aggregations is by roughly equating it to the SQL GROUP BY and the SQL aggregate functions. In Elasticsearch, you have the ability to execute searches returning hits and at the same time return aggregated results separate from the hits all in one response. This is very powerful and efficient in the sense that you can run queries and multiple aggregations and get the results back of both (or either) operations in one shot avoiding network roundtrips using a concise and simplified API.

To start with, this example groups all the accounts by state, and then returns the top 10 (default) states sorted by count descending (also default):

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In SQL, the above aggregation is similar in concept to:

[source,sh]
--------------------------------------------------
SELECT state, COUNT(*) FROM bank GROUP BY state ORDER BY COUNT(*) DESC LIMIT 10;
--------------------------------------------------

And the response (partially shown):

[source,js]
--------------------------------------------------
{
  "took": 29,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped" : 0,
    "failed": 0
  },
  "hits" : {
     "total" : {
        "value": 1000,
        "relation": "eq"
     },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "group_by_state" : {
      "doc_count_error_upper_bound": 20,
      "sum_other_doc_count": 770,
      "buckets" : [ {
        "key" : "ID",
        "doc_count" : 27
      }, {
        "key" : "TX",
        "doc_count" : 27
      }, {
        "key" : "AL",
        "doc_count" : 25
      }, {
        "key" : "MD",
        "doc_count" : 25
      }, {
        "key" : "TN",
        "doc_count" : 23
      }, {
        "key" : "MA",
        "doc_count" : 21
      }, {
        "key" : "NC",
        "doc_count" : 21
      }, {
        "key" : "ND",
        "doc_count" : 21
      }, {
        "key" : "ME",
        "doc_count" : 20
      }, {
        "key" : "MO",
        "doc_count" : 20
      } ]
    }
  }
}
--------------------------------------------------
// TESTRESPONSE[s/"took": 29/"took": $body.took/]

We can see that there are 27 accounts in `ID` (Idaho), followed by 27 accounts
in `TX` (Texas), followed by 25 accounts in `AL` (Alabama), and so forth.

Note that we set `size=0` to not show search hits because we only want to see the aggregation results in the response.

Building on the previous aggregation, this example calculates the average account balance by state (again only for the top 10 states sorted by count in descending order):

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Notice how we nested the `average_balance` aggregation inside the `group_by_state` aggregation. This is a common pattern for all the aggregations. You can nest aggregations inside aggregations arbitrarily to extract pivoted summarizations that you require from your data.

Building on the previous aggregation, let's now sort on the average balance in descending order:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword",
        "order": {
          "average_balance": "desc"
        }
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example demonstrates how we can group by age brackets (ages 20-29, 30-39, and 40-49), then by gender, and then finally get the average account balance, per age bracket, per gender:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_age": {
      "range": {
        "field": "age",
        "ranges": [
          {
            "from": 20,
            "to": 30
          },
          {
            "from": 30,
            "to": 40
          },
          {
            "from": 40,
            "to": 50
          }
        ]
      },
      "aggs": {
        "group_by_gender": {
          "terms": {
            "field": "gender.keyword"
          },
          "aggs": {
            "average_balance": {
              "avg": {
                "field": "balance"
              }
            }
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

There are many other aggregations capabilities that we won't go into detail here. The {ref}/search-aggregations.html[aggregations reference guide] is a great starting point if you want to do further experimentation.

[[getting-started-conclusion]]
== Conclusion

Elasticsearch is both a simple and complex product. We've so far learned the basics of what it is, how to look inside of it, and how to work with it using some of the REST APIs. Hopefully this tutorial has given you a better understanding of what Elasticsearch is and more importantly, inspired you to further experiment with the rest of its great features!
